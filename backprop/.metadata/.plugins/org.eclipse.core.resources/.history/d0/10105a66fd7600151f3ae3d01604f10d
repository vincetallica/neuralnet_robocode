package NeuralNet;

import Common.CommonInterface;

public interface NeuralNetInterface extends CommonInterface {

	double bias = 1.0; // The input for each neurons bias weight
	int NUM_OUTPUTS = 1;
	int NUM_INPUTS = 3;
	int NUM_HIDDEN = 4;
	int NUM_EPOCH = 200;
	int NUM_PATTERNS = 4;

    double LR = 0.01;      // Learning rate, input to hidden weights
    double MT = 0.01;		// momentum term
    double argLowerB = 0; //lowerbound of sigmoid by output neuron only
    double argUpperB = 0; //upperbound of custom sigmoid by output neuron only

    static double hiddenVal[] = new double[NUM_HIDDEN];    // u = Hidden node outputs.

    public static double weightsIH[][] =  new double[NUM_INPUTS][NUM_HIDDEN]; // Input to Hidden weights.
    public static double weightsHO[][] = new double[NUM_OUTPUTS][NUM_HIDDEN];    // Hidden to Output weights.

    public static int trainInputs[][] = new int[NUM_PATTERNS][NUM_INPUTS];
    public static int trainOutput[] = new int[NUM_PATTERNS];      // "Actual" output values.

   /**
 * Constructor. (Cannot be declared in an interface, but your implementation will need one)
 * @param argNumInputs The number of inputs in your input vector
 * @param argNumHidden The number of hidden neurons in your hidden layer. Only a single hidden layer is supported
 * @param argLearningRate The learning rate coefficient
 * @param argMomentumTerm The momentum coefficient
 * @param argA Integer lower bound of sigmoid used by the output neuron only.
 * @param arbB Integer upper bound of sigmoid used by the output neuron only.

 public abstract NeuralNet (
    int argNumInputs,
	int argNumHidden,
	double argLearningRate,
	double argMomentumTerm,
	double argA,
	double argB );
	*/
   /**
    * 
* Return a bipolar sigmoid of the input X
 * @param x The input
 * @return f(x) = 2 / (1+e(-x)) - 1
 */
 public double sigmoid(double x);

 /**
* This method implements a general sigmoid with asymptotes bounded by (a,b)
 * @param x The input
 * @return f(x) = b_minus_a / (1 + e(-x)) - minus_a
 */
 public double customSigmoid(double x);

 /**
 * Initialize the weights to random values.
 * For say 2 inputs, the input vector is [0] & [1]. We add [2] for the bias.
 * hence 3 inputs (inlcuding bias weight)
 * 
 * Like wise for hidden units. For say 2 hidden units which are stored in an array.
 * [0] & [1] are the hidden & [2] the bias.
 * ?We also initialise the last weight change arrays. This is to implement the alpha term.
 */
 public void initializeWeights();

 /**
 * Initialize the weights to 0.
 */
 public void zeroWeights();


 } // End of public interface NeuralNetInterface